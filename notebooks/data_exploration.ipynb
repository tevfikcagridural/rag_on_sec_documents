{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "data_folder_path = '../data/raw/'\n",
    "doc_paths = os.listdir(data_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Data Colletion & Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "\n",
    "documents = {}\n",
    "for doc in doc_paths:\n",
    "    doc_name = doc.replace(' ','_'). replace('.pdf', '')\n",
    "    documents[doc_name] = loader.load(file_path= data_folder_path + doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2023_Q2_INTC', '2023_Q3_INTC', '2023_Q3_AMZN', '2023_Q2_AMZN', '2022_Q3_AAPL', '2023_Q1_AAPL', '2023_Q2_MSFT', '2023_Q3_MSFT', '2023_Q2_NVDA', '2023_Q3_NVDA', '2023_Q1_NVDA', '2022_Q3_NVDA', '2023_Q1_MSFT', '2022_Q3_MSFT', '2023_Q2_AAPL', '2023_Q3_AAPL', '2022_Q3_AMZN', '2022_Q3_INTC', '2023_Q1_AMZN', '2023_Q1_INTC'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_pages': 28, 'file_path': '../data/raw/2023 Q2 AAPL.pdf', 'source': '1'}\n",
      "{'total_pages': 28, 'file_path': '../data/raw/2023 Q2 AAPL.pdf', 'source': '2'}\n"
     ]
    }
   ],
   "source": [
    "print(documents['2023_Q2_AAPL'][0].metadata)\n",
    "print(documents['2023_Q2_AAPL'][1].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1037\n"
     ]
    }
   ],
   "source": [
    "total_pages = 0\n",
    "for key, doc in documents.items():\n",
    "    total_pages += doc[0].metadata['total_pages']\n",
    "print(total_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Data Cleaning & Prerpocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_pages': 28, 'file_path': '../data/raw/2023 Q2 AAPL.pdf', 'source': '9'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['2023_Q2_AAPL'][8].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple Inc.\\nNotes to Condensed Consolidated Financial Statements (Unaudited)\\nNote 1 – Summary of Significant Accounting Policies\\nBasis of Presentation and Preparation\\nThe condensed consolidated financial statements include the accounts of Apple Inc. and its wholly owned subsidiaries (collectively “Apple” or the “Company”).\\nIntercompany accounts and transactions have been eliminated. In the opinion of the Company’s management, the condensed consolidated financial statements\\nreflect all adjustments, which are normal and recurring in nature, necessary for fair financial statement presentation. The preparation of these condensed\\nconsolidated financial statements and accompanying notes in conformity with U.S. generally accepted accounting principles requires management to make\\nestimates and assumptions that affect the amounts reported. Actual results could differ materially from those estimates. Certain prior period amounts in the\\ncondensed consolidated financial statements and accompanying notes have been reclassified to conform to the current period’s presentation. These condensed\\nconsolidated financial statements and accompanying notes should be read in conjunction with the Company’s annual consolidated financial statements and\\naccompanying notes included in its Annual Report on Form 10-K for the fiscal year ended September\\xa024, 2022.\\nThe Company’s fiscal year is the 52- or 53-week period that ends on the last Saturday of September. An additional week is included in the first fiscal quarter\\nevery five or six years to realign the Company’s fiscal quarters with calendar quarters, which occurred in the first fiscal quarter of 2023. The Company’s fiscal\\nyears 2023 and 2022 span 53 and 52 weeks, respectively. Unless otherwise stated, references to particular years, quarters, months and periods refer to the\\nCompany’s fiscal years ended in September and the associated quarters, months and periods of those fiscal years.\\nEarnings Per Share\\nThe following table shows the computation of basic and diluted earnings per share for the three- and six-month periods ended April 1, 2023 and March 26, 2022\\n(net income in millions and shares in thousands):\\nThree Months Ended\\nSix Months Ended\\nApril 1,\\n2023\\nMarch 26,\\n2022\\nApril 1,\\n2023\\nMarch 26,\\n2022\\nNumerator:\\nNet income\\n$\\n24,160\\xa0\\n$\\n25,010\\xa0\\n$\\n54,158\\xa0\\n$\\n59,640\\xa0\\nDenominator:\\nWeighted-average basic shares outstanding\\n15,787,154\\xa0\\n16,278,802\\xa0\\n15,839,939\\xa0\\n16,335,263\\xa0\\nEffect of dilutive securities\\n59,896\\xa0\\n124,514\\xa0\\n61,445\\xa0\\n126,041\\xa0\\nWeighted-average diluted shares\\n15,847,050\\xa0\\n16,403,316\\xa0\\n15,901,384\\xa0\\n16,461,304\\xa0\\nBasic earnings per share\\n$\\n1.53\\xa0\\n$\\n1.54\\xa0\\n$\\n3.42\\xa0\\n$\\n3.65\\xa0\\nDiluted earnings per share\\n$\\n1.52\\xa0\\n$\\n1.52\\xa0\\n$\\n3.41\\xa0\\n$\\n3.62\\xa0\\nApproximately 48\\xa0million restricted stock units (“RSUs”) were excluded from the computation of diluted earnings per share for the six months ended April 1, 2023\\nbecause their effect would have been antidilutive.\\nApple Inc. | Q2 2023 Form 10-Q | 6\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['2023_Q2_AAPL'][8].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = data_folder_path + '2023 Q2 AAPL.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2.1 PDF Parsing & Chunking & Token Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Parsing with `pymupdf4llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "with open('../data/interim/pymupdf4llm_2023_Q2_AAPL.md', 'w+') as f:\n",
    "    f.write(pymupdf4llm.to_markdown(sample_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT**  \n",
    "> Fails to convert tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Parsing with LlamaParse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id cac11eca-5289-4430-a4fe-efa3f0794521\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "LLAMA_CLOUD_API_KEY = os.getenv('LLAMA_CLOUD_API_KEY')\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMA_CLOUD_API_KEY,\n",
    "    result_type='markdown',\n",
    ")\n",
    "\n",
    "md_text = parser.load_data(sample_file)\n",
    "\n",
    "with open('../data/interim/llamaparse_2023_Q2_APPL.md', 'w') as f:\n",
    "    f.write(md_text[0].get_content())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT**  \n",
    "> Extracts tables better than `pymupdf4llm` but gets greatly confused with the table structure and sometimes even do not extract some text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Parsing with Unstructured API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models import shared\n",
    "from unstructured_client.models.errors import SDKError\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "client = UnstructuredClient(\n",
    "    api_key_auth=os.getenv('UNSTRUCTURED_API_KEY')\n",
    ")\n",
    "\n",
    "def unstructured_api_call(\n",
    "    file_path: str, \n",
    "    strategy: str='auto', \n",
    "    chunking_strategy: str|None=None, \n",
    "    multipage_sections: bool|None=None, \n",
    "    max_characters: int|None=None,\n",
    "    ):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        files=shared.Files(\n",
    "            content=f.read(),\n",
    "            file_name=file_path,\n",
    "        )\n",
    "\n",
    "    req = shared.PartitionParameters(\n",
    "        files=files, \n",
    "        strategy=strategy,\n",
    "        chunking_strategy=chunking_strategy,\n",
    "        multipage_sections=multipage_sections,\n",
    "        max_characters=max_characters\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        return client.general.partition(req)\n",
    "    except SDKError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = unstructured_api_call(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Footer': 13,\n",
      " 'Header': 3,\n",
      " 'ListItem': 1,\n",
      " 'NarrativeText': 169,\n",
      " 'Table': 35,\n",
      " 'Title': 101,\n",
      " 'UncategorizedText': 3}\n"
     ]
    }
   ],
   "source": [
    "types = {}\n",
    "for element in response.elements:\n",
    "    if element['type'] not in types:\n",
    "        types[element['type']] = 0\n",
    "    types[element['type']] += 1\n",
    "pprint(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manually type count results:**   \n",
    "- Table: 30\n",
    "- Footer: 22\n",
    "- Header: 0\n",
    "- Image: 1 *ps. Apple Logo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an html copy of the sample file for manual comparison\n",
    "page_html = ''\n",
    "for element in response.elements:\n",
    "        try:\n",
    "            if element['type'] not in ['Header', 'Footer']:\n",
    "                html = element['metadata']['text_as_html']\n",
    "                page_html += html\n",
    "        except KeyError:\n",
    "            text = element['text']\n",
    "            if element['type'] == 'Title':\n",
    "                page_html += f'<b>{text}</b>'\n",
    "            else:\n",
    "                page_html += text\n",
    "\n",
    "with open('../data/interim/unstructured_2023_Q2_APPL.html', 'w') as f:\n",
    "    f.write(page_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT**  \n",
    ">Works great!. Yet this is an API service with [100-page cap per month](https://docs.unstructured.io/api-reference/api-services/free-api#free-unstructured-api-limitationst). Paid and self hosted alternatives available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Parsing with LangChain\n",
    "Load one document from each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "sample_files = [file for file in os.listdir(data_folder_path) if file.startswith('2023')]\n",
    "sample_docs = []\n",
    "for file in sample_files:\n",
    "    doc_temp = PyMuPDFLoader(data_folder_path + file)\n",
    "    sample_docs.append(doc_temp.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPL Notes:**\n",
    "- Can skip first 3 pages\n",
    "- Doesn't have header\n",
    "- Has footer. Sample: `Apple Inc. | Q3 2023 Form 10-Q | 1` Final number of this is page number\n",
    "-  Can remove the page starting with `SIGNATURE`\n",
    "- Texts after `Exhibit 31.1` may be generic. Check and remove if so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def doc_cleaner(\n",
    "    document, \n",
    "    skip_first: int|None=None, \n",
    "    cleaning_patterns: List|None=None, \n",
    "    remove_page_identifiers: List|None=None, \n",
    "    final_page_identifier: str|None=None\n",
    "    ) -> list:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        document :\n",
    "        skip_first (int|Optional): Number of pages to skip at the begining of the document\n",
    "        cleaning_patterns (List|Optional): List of regex patters to detect and remove the corresponding text \n",
    "        remove_page_identifiers (List|Optional): List of regex patters to detect and remove whole pages. \n",
    "        final_page_identifier (str|Optional): The regex pattern to detect final page. This page and the following pages will be removed\n",
    "        \n",
    "    Returns\n",
    "        document_page or None\n",
    "    \"\"\"\n",
    "    \n",
    "    def text_remover(pattern, string):\n",
    "        match = re.findall(pattern, string)\n",
    "        if match:\n",
    "            return string.replace(match[0], '')\n",
    "        else:\n",
    "            return string\n",
    "    \n",
    "    final_page_num = 1e10\n",
    "    cleaned_doc = []\n",
    "    \n",
    "    for page in document:\n",
    "        \n",
    "        # Skip first n pages\n",
    "        if skip_first and page.metadata['page'] <= skip_first:\n",
    "            continue\n",
    "\n",
    "        # Remove texts with matching given patterns\n",
    "        if cleaning_patterns:\n",
    "            for pattern in cleaning_patterns:\n",
    "                page.page_content = text_remover(pattern, page.page_content)\n",
    "        \n",
    "        # Remove pages with matching given identifiers\n",
    "        if remove_page_identifiers: \n",
    "            for identifier in remove_page_identifiers:\n",
    "                if re.findall(identifier, page.page_content):\n",
    "                    continue\n",
    "            \n",
    "        # Remove pages on and after final_page_identifier match\n",
    "        if final_page_identifier:\n",
    "            if re.findall(final_page_identifier, page.page_content): # type: ignore\n",
    "                final_page_num = page.metadata['page']\n",
    "\n",
    "        if page.metadata['page'] >= final_page_num:\n",
    "            continue\n",
    "        \n",
    "        cleaned_doc.append(page)\n",
    "        \n",
    "    return cleaned_doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaning lenght for APPL docs\n",
      "46\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample cleaning lenght for APPL docs\")\n",
    "print(len(sample_docs[4]))\n",
    "print(len(doc_cleaner(\n",
    "    sample_docs[4], \n",
    "    skip_first=2, \n",
    "    cleaning_patterns=[r'(Apple Inc. \\| Q[0-9]{1} [0-9]{4} Form 10-Q \\| [0-9]+)'], \n",
    "    remove_page_identifiers=['SIGNATURE'],\n",
    "    final_page_identifier='Exhibit 31.1', \n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleanning length for AMZN docs\n",
      "51\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print('Sample cleanning length for AMZN docs')\n",
    "print(len(sample_docs[2]))\n",
    "print(len(doc_cleaner(\n",
    "    sample_docs[2],\n",
    "    skip_first=1,\n",
    "    cleaning_patterns=[r'Table of Contents'],\n",
    "    final_page_identifier='PART II. OTHER INFORMATION',\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleanning length for INTC docs\n",
      "54\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print('Sample cleanning length for INTC docs')\n",
    "print(len(sample_docs[1]))\n",
    "print(len(doc_cleaner(\n",
    "    sample_docs[1],\n",
    "    skip_first=3,\n",
    "    cleaning_patterns=[r'Table of Contents'],\n",
    "    final_page_identifier='Exhibit 31.1',\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleanning length for MSFT docs\n",
      "74\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print('Sample cleanning length for MSFT docs')\n",
    "print(len(sample_docs[5]))\n",
    "print(len(doc_cleaner(\n",
    "    sample_docs[5],\n",
    "    skip_first=1,\n",
    "    cleaning_patterns=[r'PART (I|II) Item [0-9]{1,2}'],\n",
    "    final_page_identifier='Exhibit 31.1',\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NVDA Notes:**\n",
    "- Can skip first 2 pages\n",
    "- Has header: One type: `NVIDIA CORPORATION AND SUBSIDIARIES\n",
    "NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
    "(Unaudited)`\n",
    "- Doesn't have footer\n",
    "- Has page number. But no unique identifier. Excluding them may not be possible\n",
    "- Texts after `Exhibit 31.1` may be generic. Check and remove if so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleanning length for NVDA docs\n",
      "51\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print('Sample cleanning length for NVDA docs')\n",
    "print(len(sample_docs[7]))\n",
    "print(len(doc_cleaner(\n",
    "    sample_docs[7],\n",
    "    skip_first=1,\n",
    "    cleaning_patterns=[r\"NVIDIA CORPORATION AND SUBSIDIARIES NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued) (Unaudited)\"],\n",
    "    final_page_identifier='Exhibit 31.1',\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='mxbai-embed-large:latest')\n",
    "text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type='percentile')\n",
    "\n",
    "chunked_docs = all_docs.split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Token Count Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 OpenAI Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_model: str = 'gpt-3.5-turbo') -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens for GPT3.5:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "667132"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total number of tokens for GPT3.5:\")\n",
    "sum([num_tokens_from_string(doc.page_content) for doc in chunked_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Open Source Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Initial Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Text Length Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Section Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Linguistic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4.1 Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5.1 Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5.2 PaCMAP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Section Length Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Content Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 7.2 Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Table Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Graph Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 7.5 Image Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Information Retrieval Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 8.1 Query Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 8.2 Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.Advanced Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 9.1 Embedding Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 9.2 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Analysis Notes\n",
    "* Pure Reader:  \n",
    "  1. Paragraphs seperated by '\\n\\n' and '\\n' is just newline. If recursive chunking method will be used the order is ` ['\\n\\n', (?<=\\.\\n[A-Z]), '.', '\\n\\t', '\\n', ' ', ''] `.\n",
    "  2. Values within the tables are seperated by `\\xa0`, they need to be raplaced by either `|` or `' '`\n",
    "* `pymupdf4llms` library fails to convert tables\n",
    "* LlamaParse constructs some tables. However, fails greatly on some intended tables. Which documents have lots.\n",
    "* Unstrucred works best within these alternatives. Yet to keep in mind that it has a limit of [100-pages cap per month](https://docs.unstructured.io/api-reference/api-services/free-api#free-unstructured-api-limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Manual Controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPL Notes:**\n",
    "- Can skip first 3 pages\n",
    "- Doesn't have header\n",
    "- Has footer. Sample: `Apple Inc. | Q3 2023 Form 10-Q | 1` Final number of this is page number\n",
    "-  Can remove the page starting with `SIGNATURE`\n",
    "- Texts after `Exhibit 31.1` may be generic. Check and remove if so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AMZN Notes:**\n",
    "- Can skip first 2 pages\n",
    "- Has header. Sample: `Table of Contents`\n",
    "- Has page number. But there may not be a significant identifier for them. Check a sample content if it contains a form of line or multiple newlines.\n",
    "- Texts after `PART II. OTHER INFORMATION` may be generic. Check and remove if so\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTC Notes:**\n",
    "- Can skip first 4 pages\n",
    "- Has header. Sample: `Table of Contents`\n",
    "- Has footer and page number. But no unique identifier. Excluding them may not be possible\n",
    "- Texts after `Exhibit 31.1` may be generic. Check and remove if so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSFT Notes:**\n",
    "- Can skip first 2 pages\n",
    "- Has header: Sample: `PART II Item 6`\n",
    "- Doesn't have footer.\n",
    "- Has page number. But no unique identifier. Exculing them may not be possible\n",
    "- Texts after `Exhibit 31.1` may be generic. Check and remove if so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NVDA Notes:**\n",
    "- Can skip first 2 pages\n",
    "- Has header: One type: `NVIDIA CORPORATION AND SUBSIDIARIES\n",
    "NOTES TO CONDENSED CONSOLIDATED FINANCIAL STATEMENTS (Continued)\n",
    "(Unaudited)`\n",
    "- Doesn't have footer\n",
    "- Has page number. But no unique identifier. Excluding them may not be possible\n",
    "- Texts after `Exhibit 31.1` may be generic. Check and remove if so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragonsec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
